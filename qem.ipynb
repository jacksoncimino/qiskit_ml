{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73289b27",
   "metadata": {},
   "source": [
    "**Generate data with qiskit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15a9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.circuit.random import random_circuit\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_ibm_runtime import EstimatorV2\n",
    "from qiskit_ibm_runtime.fake_provider import FakeManilaV2\n",
    "from qiskit_aer import AerSimulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a98ec",
   "metadata": {},
   "source": [
    "The following cell is a simple experiment to show the difference of using an ideal vs noisy simulator. We are trying to map the noisy expectation value to the ideal expectation value with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a076ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal expectation value: 1.0\n",
      "Noisy expectation value: 0.6904296875\n"
     ]
    }
   ],
   "source": [
    "# Initialize circuit with 2 qubits and create bell state (we still use 5 qubits to match backend)\n",
    "qc = QuantumCircuit(5)\n",
    "qc.h(0)\n",
    "qc.cx(0,1)\n",
    "\n",
    "# create observable ZZ for expectation value estimation\n",
    "observable = SparsePauliOp(\"ZZZZZ\")\n",
    "\n",
    "backend = FakeManilaV2()\n",
    "\n",
    "# use fake backend that simulates noise based on a real IBM quantum computer\n",
    "noisy_backend = AerSimulator.from_backend(backend)\n",
    "# use simulated backend for target values\\\n",
    "ideal_backend = AerSimulator()\n",
    "\n",
    "# transpile circuit for both backends\n",
    "transpiled_circuit = transpile(qc, backend)\n",
    "\n",
    "# create estimators for both backends\n",
    "noisy_estimator = EstimatorV2(noisy_backend)\n",
    "ideal_estimator = EstimatorV2(ideal_backend)\n",
    "\n",
    "# run circuits\n",
    "noisy_result = noisy_estimator.run([(transpiled_circuit, observable)]).result()\n",
    "\n",
    "ideal_result = ideal_estimator.run([(transpiled_circuit, observable)]).result()\n",
    "\n",
    "print(f\"Ideal expectation value: {ideal_result[0].data.evs}\")\n",
    "print(f\"Noisy expectation value: {noisy_result[0].data.evs}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052f749",
   "metadata": {},
   "source": [
    "The ideal expectation value is 1, but our noisy (realistic) backend got a different value. We want to build a model that can map this value to its ideal expectation, given the size and depth of the state. Now we do the same thing, but we randomize our circuits with various depths, and generate 1000s of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc52d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build observables for specific backend\n",
    "def build_observables_for_backend(backend):\n",
    "    n = backend.configuration().num_qubits\n",
    "    # get coupling map as list of directed edges; convert to undirected unique pairs\n",
    "    try:\n",
    "        coupling = backend.configuration().coupling_map\n",
    "    except Exception:\n",
    "        coupling = getattr(backend.configuration(), 'coupling_map', [])\n",
    "    # Normalize to unique undirected edges\n",
    "    edge_set = set()\n",
    "    for a, b in coupling:\n",
    "        edge_set.add(tuple(sorted((a, b))))\n",
    "    edges = sorted(edge_set)\n",
    "\n",
    "    observables = []\n",
    "\n",
    "    # single Z observables ('IIZII', etc)\n",
    "    for i in range(n):\n",
    "        pauli = ['I'] * n\n",
    "        pauli[i] = 'Z'\n",
    "        observables.append(SparsePauliOp(''.join(pauli)))\n",
    "\n",
    "    # two qubit ZZ observables for each neighboring pair of qubits\n",
    "    for i, j in edges:\n",
    "        pauli = ['I'] * n\n",
    "        pauli[i] = 'Z'\n",
    "        pauli[j] = 'Z'\n",
    "        observables.append(SparsePauliOp(''.join(pauli)))\n",
    "    \n",
    "    return observables\n",
    "\n",
    "# Initialize circuit with 5 qubits\n",
    "num_qubits = 5\n",
    "depths = [2,4,6,8,10,12,14, 16, 18, 20]\n",
    "num_circuits_per_depth = 560\n",
    "\n",
    "# use fake backend that simulates noise based on a real IBM quantum computer\n",
    "backend = FakeManilaV2()\n",
    "noisy_backend = AerSimulator.from_backend(backend)\n",
    "# use simulated backend for target values\\\n",
    "ideal_backend = AerSimulator()\n",
    "\n",
    "observables = build_observables_for_backend(noisy_backend)\n",
    "# create estimators for both backends\n",
    "noisy_estimator = EstimatorV2(noisy_backend)\n",
    "ideal_estimator = EstimatorV2(ideal_backend)\n",
    "\n",
    "circuits = []\n",
    "noisy_result = []\n",
    "ideal_result = []\n",
    "# generate random circuits of varying depths. Generate many circuits per depth\n",
    "for depth in depths:\n",
    "    for i in range(num_circuits_per_depth):\n",
    "        # create a random circuit with 2 qubit gates. Use seed to produce same data each run\n",
    "        qc = random_circuit(num_qubits=num_qubits, depth=depth, max_operands=2, measure=False, seed=i)\n",
    "        circuits.append(qc)\n",
    "        \n",
    "# transpile all circuits for both backends at once for better performance\n",
    "transpiled_circuits = transpile(circuits, backend)\n",
    "\n",
    "# run circuits\n",
    "for i in range(len(circuits)):\n",
    "    noisy_result.append(noisy_estimator.run([(transpiled_circuits[i], observables)]).result()[0].data.evs)\n",
    "    ideal_result.append(ideal_estimator.run([(transpiled_circuits[i], observables)]).result()[0].data.evs)\n",
    "\n",
    "# generate our data matrices\n",
    "X = np.asarray(noisy_result).reshape(len(circuits), len(observables))\n",
    "Y = np.asarray(ideal_result).reshape(len(circuits), len(observables))\n",
    "\n",
    "# add more features for X\n",
    "depths = np.array([qc.depth() for qc in circuits]).reshape(-1, 1)  # shape (N,1)\n",
    "# standardize\n",
    "scaler = StandardScaler()\n",
    "depths_std = scaler.fit_transform(depths)\n",
    "\n",
    "X_with_depth = np.concatenate([X, depths_std], axis=1)\n",
    "\n",
    "# Save as csv so we don't have to recompute\n",
    "# rename files if you want to keep data\n",
    "np.savetxt('X.csv', X_with_depth, delimiter=',', fmt='%f')\n",
    "np.savetxt('Y.csv', Y, delimiter=',', fmt='%f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772565e",
   "metadata": {},
   "source": [
    "Below is code for a denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b67e9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2113cb",
   "metadata": {},
   "source": [
    "Autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6254b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoising Autoencoder model\n",
    "class DAE(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, hidden_size)\n",
    "        )\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size-1) #exclude depth feature\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return self.decoder(encoded)\n",
    "    \n",
    "\n",
    "# training function\n",
    "def train(model, epochs, optimizer, criterion, train_loader):\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss/len(train_loader):.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41faa45b",
   "metadata": {},
   "source": [
    "Load data and convert it to tensors and torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdcf53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "X = np.loadtxt('X_5600.csv', delimiter=',')\n",
    "Y = np.loadtxt('Y_5600.csv', delimiter=',')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle=True)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train,Y_train)\n",
    "test_dataset = TensorDataset(X_test,Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab577af",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1ba4d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.194723\n",
      "Epoch 10, Loss: 0.010010\n",
      "Epoch 20, Loss: 0.007532\n",
      "Epoch 30, Loss: 0.006735\n",
      "Epoch 40, Loss: 0.006349\n",
      "Epoch 50, Loss: 0.006120\n",
      "Epoch 60, Loss: 0.005907\n",
      "Epoch 70, Loss: 0.005680\n",
      "Epoch 80, Loss: 0.005467\n",
      "Epoch 90, Loss: 0.005338\n",
      "Epoch 100, Loss: 0.005272\n",
      "Epoch 110, Loss: 0.005179\n",
      "Epoch 120, Loss: 0.005162\n",
      "Epoch 130, Loss: 0.005120\n",
      "Epoch 140, Loss: 0.004974\n",
      "Epoch 150, Loss: 0.004894\n",
      "Epoch 160, Loss: 0.004871\n",
      "Epoch 170, Loss: 0.004829\n",
      "Epoch 180, Loss: 0.004820\n",
      "Epoch 190, Loss: 0.004737\n",
      "Epoch 200, Loss: 0.004698\n",
      "Epoch 210, Loss: 0.004656\n",
      "Epoch 220, Loss: 0.004626\n",
      "Epoch 230, Loss: 0.004534\n",
      "Epoch 240, Loss: 0.004560\n",
      "Epoch 250, Loss: 0.004471\n",
      "Epoch 260, Loss: 0.004477\n",
      "Epoch 270, Loss: 0.004429\n",
      "Epoch 280, Loss: 0.004411\n",
      "Epoch 290, Loss: 0.004323\n",
      "Epoch 300, Loss: 0.004426\n",
      "Epoch 310, Loss: 0.004261\n",
      "Epoch 320, Loss: 0.004249\n",
      "Epoch 330, Loss: 0.004186\n",
      "Epoch 340, Loss: 0.004257\n",
      "Epoch 350, Loss: 0.004211\n",
      "Epoch 360, Loss: 0.004165\n",
      "Epoch 370, Loss: 0.004133\n",
      "Epoch 380, Loss: 0.004062\n",
      "Epoch 390, Loss: 0.004179\n"
     ]
    }
   ],
   "source": [
    "# define our model, epochs, optimizer, and criterion\n",
    "model = DAE(input_size=X_train.shape[1], hidden_size=8).to(device)\n",
    "epochs = 400\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# train\n",
    "train(model, epochs, optimizer, criterion, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508b629",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02d0f9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.004813066218048334\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test.to(device)).cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE:\", mean_squared_error(Y_test.numpy(), preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a749c",
   "metadata": {},
   "source": [
    "Compare against some simple baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6432b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Identity): 0.011287\n",
      "MSE (Mean Predictor): 0.219958\n",
      "MSE (Linear Regression): 0.003994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume these are your datasets\n",
    "# X_train, Y_train: training (noisy + depth, ideal)\n",
    "# X_test, Y_test: test set\n",
    "# If you haven't added depth yet, X_train = noisy observables only\n",
    "\n",
    "# --------------------------\n",
    "# Baseline 1: Identity (No Denoising)\n",
    "# --------------------------\n",
    "identity_pred = X_test[:, :-1]  # ignore depth if appended at last column\n",
    "mse_identity = mean_squared_error(Y_test, identity_pred)\n",
    "print(f\"MSE (Identity): {mse_identity:.6f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Baseline 2: Mean Predictor\n",
    "# --------------------------\n",
    "mean_Y = Y_train.mean(axis=0)\n",
    "mean_pred = np.tile(mean_Y, (Y_test.shape[0], 1))\n",
    "mse_mean = mean_squared_error(Y_test, mean_pred)\n",
    "print(f\"MSE (Mean Predictor): {mse_mean:.6f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Baseline 3: Linear Regression\n",
    "# --------------------------\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, Y_train)\n",
    "linreg_pred = linreg.predict(X_test)\n",
    "mse_linreg = mean_squared_error(Y_test, linreg_pred)\n",
    "print(f\"MSE (Linear Regression): {mse_linreg:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63bc389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions and targets\n",
    "np.savetxt('Y_pred.csv', preds, delimiter=',', fmt='%f')\n",
    "np.savetxt('Y_test.csv', Y_test.numpy(), delimiter=',', fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
